---
title: "Spatiotemporal Logistic Regression Modeling"
author: "TJ Sipin"
date: "2023-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=T)
library(tidyr)
library(sf)
library(dplyr)
library(rsample)
library(caret)
library(vip)
```

## Data

```{r}
data <- readRDS("~/peregrine_amazon/data/annual/aad_2021_forests.rds") %>% 
  # select only general and forest variables
  select(Code:Population, min_Precip:max_Precip_Month, 
         forest_density, SWChange_abs:lat, -AvgRad, -StableLights) %>% 
  mutate(CL = ifelse(CL > 0, 1, 0) %>% 
           as.factor()) %>% 
  filter(!is.na(CL),
         !is.na(forest_density)) %>% 
  mutate(Country = as.factor(Country))

predictor_names <- data %>% 
  select(-c(Code, Name, Country, CL, fold)) %>% 
  names()
```

## Spaciotemporal CV

We try different approaches to troubleshoot problems that we may encounter. The basic structure of each approach is simple. Use a spatiotemporal cross validation approach to test the model on a linear regression model. For every spatiotemporal fold $ij$, we use all data outside of the fold $ij$ as the training data and test it on the data inside of the $ij$ fold. Then we gather the optimal threshold using a ROAUC curve on some validation data and finally test it on the untouched testing data. 

### Naive approach

Below, we just create a model on all the data without any spatiotemporal cross validation to see the top explanatory variables and compare with each approach.

```{r}
set.seed(123)

full.predictor_names <- data %>% 
  select(-c(Name, Country, Code, CL, fold)) %>% 
  names()

full.ml_formula <- as.formula(paste0("CL ~", paste(full.predictor_names, collapse=" + ")))

# get testing and training split
split <- initial_split(data, strata = CL)
full.datrain <- training(split)
full.datest <- testing(split)

# get model
full.mod <- glm(full.ml_formula, data=full.datrain, family=binomial)

# calculate out of sample model performance on full.datest
full.oob <- predict(full.mod, newdata=full.datest, type='response', na.action = na.pass)

full.temp_auc_out <- pROC::roc(response = full.datest$CL, predictor= full.oob, levels=c(0,1), auc = TRUE)

full.best_threshold_out <- pROC::coords(full.temp_auc_out, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
full.metrica.format <- data.frame(
  cbind(ifelse(full.datest$CL==1, 1, 0),
        ifelse(full.oob>=full.best_threshold_out[1,1], 1, 0))) %>% # 0.821
  mutate_all(as.factor)

colnames(full.metrica.format) <- c("labels","predictions")
rownames(full.metrica.format) <- 1:dim(full.metrica.format)[1]

full.auc_out <- pROC::roc(response = full.metrica.format$labels, predictor= full.metrica.format$predictions %>% as.numeric() - 1, levels=c(0,1), auc = TRUE)

full.metrica.format.confusionMatrix <- full.metrica.format$predictions %>% 
  confusionMatrix(positive='1', reference=full.metrica.format$labels)

full.sensitivity_out <- caret::recall(
  data = full.metrica.format.confusionMatrix$table, 
  relevant = rownames(full.metrica.format.confusionMatrix$table)[2]
) 
full.specificity_out <- caret::precision(
  data = full.metrica.format.confusionMatrix$table,
  relevant = rownames(full.metrica.format.confusionMatrix$table)[2]
)

full.var_imp_vars <- vip::vip(full.mod, num_features=20)
full.var_imp_vars
```

It seems that the socioeconomic and elevation variables sit at the top of the importance plot, as well as temperature and precipitation. We should take this with a grain of salt as it's not terribly rigorous.

Now we attempt to run a bare-bones spatiotemporal cross validation approach. Here, we make sure that each testing fold analyzed has both present and absent points. Then we take and split the training data to create a validation set for model tuning in order to save the testing set `datest` for the end. Testing on the `datrain.test` data, we get a set of most important predictors and optimal threshold to use on `datest`. We finally save the metrics to aggregate later before moving onto the next spatiotemporal fold.

```{r, eval=F}
set.seed(123)
for(j in data$Year %>% unique() %>% sort()){
  for(i in data$fold %>% unique() %>% sort()){
    print(paste(i,j, sep="_"))
    # first split using i and j
    datrain <- data %>% 
      filter(Year != j,
             fold != i)
    datest <- data %>% 
      filter(Year == j,
             fold == i)
    
    # skip fold if there is a complete imbalance of present vs. absent CL
    if(datest$CL %>% unique() %>% length() != 2){next}
    
    # second split (on datrain) before testing on datest for further validation
    datrain.split <- initial_split(datrain)
    datrain.train <- training(datrain.split)
    datrain.test <- testing(datrain.split)
  
    # get initial model
    init.mod <- glm(full.ml_formula, data=datrain.train, family=binomial)
    
    # calculate out of sample model performance on datrain.test
    oob.datrain.test <- predict(full.mod, newdata=datrain.test, type='response') 
    auc_out.datrain.test <- pROC::roc(response = datrain.test$CL, predictor= oob.datrain.test, levels=c(0,1), auc = TRUE)
    best_threshold_out.datrain.test <- pROC::coords(auc_out.datrain.test, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
    metrica.format.datrain.test <- data.frame(
      cbind(ifelse(datrain.test$CL==1, 1, 0),
            ifelse(oob.datrain.test>=best_threshold_out.datrain.test[1,1], 1, 0))) %>% 
      mutate_all(as.factor)
    
    colnames(metrica.format.datrain.test) <- c("labels","predictions")
    rownames(metrica.format.datrain.test) <- 1:dim(metrica.format.datrain.test)[1]
    
   metrica.format.datrain.test <- metrica.format.datrain.test$predictions %>% 
      confusionMatrix(positive='1', reference=metrica.format.datrain.test$labels)
    
    sensitivity_out.datrain.test <- caret::recall(
      data = metrica.format.datrain.test$table, 
      relevant = rownames(metrica.format.datrain.test$table)[2]
    ) 
    specificity_out.datrain.test <- caret::precision(
      data = metrica.format.datrain.test$table, 
      relevant = rownames(metrica.format.datrain.test$table)[2]
    )
    
    var_imp_vars.datrain.test <- vip::vip(init.mod)$data$Variable
    
    # for retrieval later
    datrain.test.metrics <- list(
      auc = auc_out.datrain.test,
      best_threshold = best_threshold_out.datrain.test,
      confusion_matrix = metrica.format.datrain.test,
      sensitivity = sensitivity_out.datrain.test,
      specificity = specificity_out.datrain.test,
      imp_vars = var_imp_vars.datrain.test)

    edited.predictor_names <- c(datrain.test.metrics$imp_vars, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss")
    ## second model on all of datrain
    edited.ml_formula <- as.formula(paste0("CL ~ ", paste(edited.predictor_names, collapse="+")))
    edited.mod <- glm(edited.ml_formula, data=datrain, family=binomial)
    
    oob.datest <- predict(edited.mod, newdata=datest, type='response')
    
    auc_out.datest <- pROC::roc(response = datest$CL, predictor= oob.datest, levels=c(0,1), auc = TRUE)
    
    best_threshold_out.datest <- pROC::coords(auc_out.datest, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
    metrica.format.datest <- data.frame(
      cbind(ifelse(datest$CL==1, 1, 0),
            ifelse(oob.datest>=best_threshold_out.datest[1,1], 1, 0))) %>% 
      mutate_all(as.factor)
    colnames(metrica.format.datest) <- c("labels","predictions")
    rownames(metrica.format.datest) <- 1:dim(metrica.format.datest)[1]
    
    metrica.format.datest <- metrica.format.datest$predictions %>% 
      confusionMatrix(positive='1', reference=metrica.format.datest$labels)
    
    sensitivity_out.datest <- caret::recall(data = metrica.format.datest$table, 
                                                  relevant = rownames(metrica.format.datest$table)[2]) 
    specificity_out.datest <- caret::precision(data = metrica.format.datest$table, 
                                                     relevant = rownames(metrica.format.datest$table)[2])
    
    var_imp_vars.datest <- vip::vip(edited.mod)$data$Variable
    
    # for retrieval later
    datest.metrics <- list(
      auc = auc_out.datest,
      best_threshold = best_threshold_out.datest,
      confusion_matrix = metrica.format.datest,
      sensitivity = sensitivity_out.datest,
      specificity = specificity_out.datest,
      imp_vars = var_imp_vars.datest)
    
    saveRDS(datrain.test.metrics,
            paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datrain.test/", i, "_", j, "_metrics.rds"))
    saveRDS(datest.metrics,
            paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datest/", i, "_", j, "_metrics.rds"))
    saveRDS(edited.mod, paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/models/", i, "_", j, "_edited_mod.rds"))
    }
}
```



### Random resampling approach

For the most part, our code does not change. But first, we need to redefine the data that we partition into `datrain` and `datest` for each spatiotemporal fold:

```{r}
set.seed(123)

# create holdout set
balanced.holdout.split <- initial_split(data, strata=fold)
balanced.holdout.train <- training(balanced.holdout.split)
balanced.holdout.test <- testing(balanced.holdout.split)

balanced_data_v1 <- balanced.holdout.train %>% 
  filter(CL == 1) %>% 
  slice_sample(n = nrow(balanced.holdout.train %>% filter(CL == 0))) %>% 
  rbind(balanced.holdout.train %>% filter(CL == 0))

dim(data)
dim(balanced_data_v1)
```

Compare the dimensions of the full data and the balanced data. The balanced training data is almost a third of the full data! The following code block uses the same structure as the one above using the full data.


```{r, eval=F}
resume_year = 2001
for(j in resume_year:max(balanced_data_v1$Year)){
  for(i in balanced_data_v1$fold %>% unique() %>% sort()){
    print(paste(i,j, sep="_"))
    # first split using i and j
    datrain <- balanced_data_v1 %>% 
      filter(Year != j,
             fold != i)
    datest <- balanced_data_v1 %>% 
      filter(Year == j,
             fold == i)
    # if the data is too imbalanced, then we skip to the next iteration i_j
    if(datest$CL %>% unique() %>% length() == 1) {next}
    
    # second split (on datrain) before testing on datest
    datrain.split <- initial_split(datrain)
    datrain.train <- training(datrain.split)
    datrain.test <- testing(datrain.split)
  
    # get initial model
    init.mod <- glm(full.ml_formula, data=datrain.train, family=binomial)
    
    # calculate out of sample model performance on datrain.test
    oob.datrain.test <- predict(init.mod, newdata=datrain.test, type='response') 
    auc_out.datrain.test <- pROC::roc(response = datrain.test$CL, predictor= oob.datrain.test, levels=c(0,1), auc = TRUE)
    best_threshold_out.datrain.test <- pROC::coords(auc_out.datrain.test, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
    metrica.format.datrain.test <- data.frame(
      cbind(ifelse(datrain.test$CL==1, 1, 0),
            ifelse(oob.datrain.test>=best_threshold_out.datrain.test[1,1], 1, 0))) %>% 
      mutate_all(as.factor)
    
    colnames(metrica.format.datrain.test) <- c("labels","predictions")
    rownames(metrica.format.datrain.test) <- 1:dim(metrica.format.datrain.test)[1]
    
   metrica.format.datrain.test <- metrica.format.datrain.test$predictions %>% 
      confusionMatrix(positive='1', reference=metrica.format.datrain.test$labels)
    
    sensitivity_out.datrain.test <- caret::recall(
      data = metrica.format.datrain.test$table, 
      relevant = rownames(metrica.format.datrain.test$table)[2]
    ) 
    specificity_out.datrain.test <- caret::precision(
      data = metrica.format.datrain.test$table, 
      relevant = rownames(metrica.format.datrain.test$table)[2]
    )
    
    var_imp_vars.datrain.test <- vip::vip(init.mod)$data$Variable
    
    
    
    # for retrieval later
    datrain.test.metrics <- list(
      auc = auc_out.datrain.test,
      best_threshold = best_threshold_out.datrain.test,
      confusion_matrix = metrica.format.datrain.test,
      sensitivity = sensitivity_out.datrain.test,
      specificity = specificity_out.datrain.test,
      imp_vars = var_imp_vars.datrain.test)
    
    edited.predictor_names <- c(datrain.test.metrics$imp_vars, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss")
    ## second model on all of datrain
    edited.ml_formula <- as.formula(paste0("CL ~ ", paste(edited.predictor_names, collapse="+")))
    edited.mod <- glm(edited.ml_formula, data=datrain, family=binomial)
    
    oob.datest <- predict(edited.mod, newdata=datest, type='response')
    
    auc_out.datest <- pROC::roc(response = datest$CL, predictor= oob.datest, levels=c(0,1), auc = TRUE)
    
    best_threshold_out.datest <- pROC::coords(auc_out.datest, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
    metrica.format.datest <- data.frame(
      cbind(ifelse(datest$CL==1, 1, 0),
            ifelse(oob.datest>=best_threshold_out.datest[1,1], 1, 0))) %>% 
      mutate_all(as.factor)
    colnames(metrica.format.datest) <- c("labels","predictions")
    rownames(metrica.format.datest) <- 1:dim(metrica.format.datest)[1]
    
    metrica.format.datest <- metrica.format.datest$predictions %>% 
      confusionMatrix(positive='1', reference=metrica.format.datest$labels)
    
    sensitivity_out.datest <- caret::recall(data = metrica.format.datest$table, 
                                                  relevant = rownames(metrica.format.datest$table)[2]) 
    specificity_out.datest <- caret::precision(data = metrica.format.datest$table, 
                                                     relevant = rownames(metrica.format.datest$table)[2])
    
    var_imp_vars.datest <- vip::vip(edited.mod)$data$Variable
    
    # for retrieval later
    datest.metrics <- list(
      auc = auc_out.datest,
      best_threshold = best_threshold_out.datest,
      confusion_matrix = metrica.format.datest,
      sensitivity = sensitivity_out.datest,
      specificity = specificity_out.datest,
      imp_vars = var_imp_vars.datest)
    
    saveRDS(datrain.test.metrics,
            paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datrain.test/", i, "_", j, "_metrics.rds"))
    saveRDS(datest.metrics,
            paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datest/", i, "_", j, "_metrics.rds"))
    saveRDS(edited.mod, paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/models/", i, "_", j, "_edited_mod.rds"))
    }
}
```

Maybe we can further choose a better resampling method since we skip a lot of iterations.

### Resampling method v2

What if we go into each spatiotemporal fold $ij$ and take $n_{ij} = $ number of observations with absent incidence in that spatiotemporal fold? Note that $n_{ij} \geq 0$.

```{r}
set.seed(123)
balanced_data_v2 <- data.frame()

for(j in balanced.holdout.train$Year %>% unique() %>% sort()){
  for(i in balanced.holdout.train$fold %>% unique() %>% sort()){
    # get ij fold subset
    data_ij <- balanced.holdout.train %>% 
      filter(Year == j,
             fold == i)
    
    # get subset of ij fold subset of only observations with no CL occurrence 
    data_ij_absent <- data_ij %>% 
      filter(CL == 0)
    
    # get the length to use when sampling to balance the data set
    n_ij <- nrow(data_ij_absent)
    
    if(n_ij == 0){next}
    
    # sample n_ij positive occurrence observations in ij fold
    n_data_ij_positive <- data_ij %>% 
      filter(CL == 1) %>% 
      nrow()
    
    if(n_data_ij_positive == 0){next}
    
    n_ij <- min(n_ij, n_data_ij_positive)
    
    data_ij_positive_sample <- data_ij %>% 
      filter(CL == 1) %>% 
      slice_sample(n=n_ij)
    
    data_ij_sample <- data_ij_positive_sample  %>% 
      rbind(data_ij_absent) # combine with subset with no CL occurrence
    
    balanced_data_v2 <- balanced_data_v2 %>% 
      rbind(data_ij_sample) # combine with main df balance_data_v2
    
    print(paste(j, i, n_ij, nrow(data_ij_positive_sample)))
  }
}
```

Let's save this balanced data for later. For now, we get our ideal threshold and predictor variables from `balanced_data_v1`. To get our threshold, we just take the mean of our thresholds provided by each fold. To get our predictor variables, we count each time the predictor variable appears in all folds. 

```{r}
# create empty vectors to be filled
threshold <- c()
imp.vars <- c()
auc <- c()
p_val <- c()

resume_year <- 2001

for(j in resume_year:max(balanced_data_v1$Year)){
  for(i in balanced_data_v1$fold %>% unique() %>% sort()){
    # make sure file exists (we excluded several folds in preliminary analysis due to a class being absent)
    if (!file.exists(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datest/", i, "_", j, "_metrics.rds"))) {next} 
    
    this.metrics <- readRDS(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datest/", i, "_", j, "_metrics.rds"))
    
    threshold <- c(threshold, this.metrics$best_threshold[[1]])
    imp.vars <- c(imp.vars, this.metrics$imp_vars)
    auc <- c(auc, this.metrics$auc$auc[[1]])
    p_val <-c(p_val, this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]])
    
  }
}

# get mean threshold
mean.threshold <- mean(threshold)

# turn imp.vars into data frame
imp.vars.df <- data.frame(var = factor(imp.vars))
# get count of each variable to rank general importance to apply to main data
v1.imp.vars.count <- imp.vars.df %>% 
  count(var) %>% 
  arrange(desc(n))

# get mean auc
mean.auc <- mean(auc)
# get mean accuracy p-value
mean.p_val <- mean(p_val)

metrics_df.balanced_v1 <- data.frame(
  metric = c('AUC', 'Accuracy P-value', 'Threshold'), 
  value = c(mean.auc, mean.p_val, mean.threshold)
)

metrics_df.balanced_v1
```

This approach gives a pretty high accuracy p-value (0.52) but decent AUC value (0.76).

#### On threshold

We can probably do better by weighting the threshold by a performance metric like no information rate (NIR) or P-value [Acc > NIR] (`this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]]`).

#### On variable importance

This method by itself can be improved by giving a weight, perhaps weighting by order of importance in each fold or a performance metric. (See above.)

For now, we can carry on with this approach and see how well it performs. We choose the 10 most popular predictor variables to use in our formula and train a model on the testing data. [TODO: continue]

```{r}
#### change v1.datrain/v1.datest -> balanced.holdout.train/v1.datest

set.seed(123)
# choose the first 10 predictor variables
v1.imp_vars <- v1.imp.vars.count %>% 
  select(var) %>% 
  filter(row_number() <= 10) %>% 
  mutate(var = as.character(var))

# get our predictor names into vector format but make sure to include the forest variables
v1.predictor_names <- c(v1.imp_vars$var, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss") %>%
  unique()

# get our threshold
v1.threshold <- metrics_df.balanced_v1 %>% 
  filter(metric=='Threshold') %>% 
  select(value) %>% 
  as.numeric()

# make the formula based on our chosen predictors
v1.ml_formula <- as.formula(paste0("CL ~", paste(v1.predictor_names, collapse=" + ")))

# get testing and training split
# split <- initial_split(data, strata = CL)
# v1.datrain <- training(split)
# v1.datest <- testing(split)

v1.datrain <- balanced.holdout.train
v1.datest <- balanced.holdout.test

# get model
v1.mod <- glm(v1.ml_formula, data=v1.datrain, family=binomial) ##

# calculate out of sample model performance on v1.datest
v1.oob <- predict(v1.mod, newdata=v1.datest, type='response', na.action = na.pass)

v1.temp_auc_out <- pROC::roc(response = v1.datest$CL, predictor= v1.oob, levels=c(0,1), auc = TRUE)

v1.best_threshold_out <- pROC::coords(v1.temp_auc_out, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
v1.metrica.format <- data.frame(
  cbind(ifelse(v1.datest$CL==1, 1, 0),
        ifelse(v1.oob>=v1.best_threshold_out[1,1], 1, 0))) %>% # v1.best_threshold_out[1,1] -> 0.8243; 
  mutate_all(as.factor)

colnames(v1.metrica.format) <- c("labels","predictions")
rownames(v1.metrica.format) <- 1:dim(v1.metrica.format)[1]

v1.auc_out <- pROC::roc(response = v1.metrica.format$labels, predictor= v1.metrica.format$predictions %>% as.ordered(), levels=c(0,1), auc = TRUE)

v1.metrica.format.confusionMatrix <- v1.metrica.format$predictions %>% 
  confusionMatrix(positive='1', reference=v1.metrica.format$labels)

v1.sensitivity_out <- caret::recall(
  data = v1.metrica.format.confusionMatrix$table, 
  relevant = rownames(v1.metrica.format.confusionMatrix$table)[2]
) 
v1.specificity_out <- caret::precision(
  data = v1.metrica.format.confusionMatrix$table, 
  relevant = rownames(v1.metrica.format.confusionMatrix$table)[2]
)

v1.var_imp_vars <- vip::vip(v1.mod, num_features = 20)
v1.var_imp_vars
```

The top variables are precipitation, elevation, and socioeconomic again.

#### Partial dependence plots

```{r v1 pdp}
v1.partial.forest_density <- pdp::partial(v1.mod, pred.var='forest_density')
pdp::plotPartial(v1.partial.forest_density)

v1.partial.forest_fragmentation <- pdp::partial(v1.mod, pred.var='forest_fragmentation')
pdp::plotPartial(v1.partial.forest_fragmentation)

v1.partial.land_use_change <- pdp::partial(v1.mod, pred.var='land_use_change')
pdp::plotPartial(v1.partial.land_use_change)

v1.partial.edge_loss <- pdp::partial(v1.mod, pred.var='edge_loss')
pdp::plotPartial(v1.partial.edge_loss)

v1.partial.Tair_f_tavg <- pdp::partial(v1.mod, pred.var='Tair_f_tavg')
pdp::plotPartial(v1.partial.Tair_f_tavg)

v1.partial.min_Precip <- pdp::partial(v1.mod, pred.var='min_Precip')
pdp::plotPartial(v1.partial.min_Precip)

summary(v1.mod)
```

This does not seem to give much deeper insight. We investigate the incorrectly classified observations:

```{r}
v1.incorrect <- balanced.holdout.test %>% 
  mutate(.pred = v1.metrica.format$predictions) %>% 
  mutate(correct = .pred==CL) %>% 
  select(Code, Name, Country, Year, CL, correct, everything())

v1.incorrect %>% summary() # there is a 1787 to 4768 split between true CL absent vs. present

# plots
v1.incorrect.important_vars <- v1.incorrect %>% 
  select(Code, Name, Year, CL, correct, v1.imp_vars$var) %>%
  mutate(log_Population = log(Population)) %>% 
  select(-Population) %>% 
  pivot_longer(cols = c(min_Elevation:log_Population))

ggplot(v1.incorrect.important_vars) +
  geom_boxplot(aes(x = correct, 
                   y = value)) +
  facet_wrap(facets='name', scales='free')

ggplot(v1.incorrect.important_vars) +
  geom_density(aes(x = value,
                   col = correct)) +
  facet_wrap(facets='name', scales='free')
```

The main takeaways here are that incorrectly identified data seem to have lower forest density, lower population, higher wind speeds than correctly identified data.

### Using resampling method v2 (balanced_data_v2)

We go on now to use the `balanced_data_v2` data set.

```{r, eval=F}
set.seed(123)
current_j = 2001

for(j in current_j:max(balanced_data_v2$Year)){
  for(i in balanced_data_v2$fold %>% unique() %>% sort()){
    print(paste(i,j, sep="_"))
    # first split using i and j
    datrain <- balanced_data_v2 %>% 
      filter(Year != j,
             fold != i)
    datest <- balanced_data_v2 %>% 
      filter(Year == j,
             fold == i)
    # if the data is too imbalanced, then we skip to the next iteration i_j
    if(datest$CL %>% unique() %>% length() == 1) {next}
    if(nrow(datest) == 0) {next}
    
    # second split (on datrain) before testing on datest
    datrain.split <- initial_split(datrain)
    datrain.train <- training(datrain.split)
    datrain.test <- testing(datrain.split)
  
    # get initial model
    init.mod <- glm(full.ml_formula, data=datrain.train, family=binomial)
    
    # calculate out of sample model performance on datrain.test
    oob.datrain.test <- predict(init.mod, newdata=datrain.test, type='response') 
    auc_out.datrain.test <- pROC::roc(response = datrain.test$CL, predictor= oob.datrain.test, levels=c(0,1), auc = TRUE)
    best_threshold_out.datrain.test <- pROC::coords(auc_out.datrain.test, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
    metrica.format.datrain.test <- data.frame(
      cbind(ifelse(datrain.test$CL==1, 1, 0),
            ifelse(oob.datrain.test>=best_threshold_out.datrain.test[1,1], 1, 0))) %>% 
      mutate_all(as.factor)
    
    colnames(metrica.format.datrain.test) <- c("labels","predictions")
    rownames(metrica.format.datrain.test) <- 1:dim(metrica.format.datrain.test)[1]
    
   metrica.format.datrain.test <- metrica.format.datrain.test$predictions %>% 
      confusionMatrix(positive='1', reference=metrica.format.datrain.test$labels)
    
    sensitivity_out.datrain.test <- caret::recall(
      data = metrica.format.datrain.test$table, 
      relevant = rownames(metrica.format.datrain.test$table)[2]
    ) 
    specificity_out.datrain.test <- caret::precision(
      data = metrica.format.datrain.test$table, 
      relevant = rownames(metrica.format.datrain.test$table)[2]
    )
    
    var_imp_vars.datrain.test <- vip::vip(init.mod)$data$Variable # picks top ten features
    
    
    # for retrieval later
    datrain.test.metrics <- list(
      auc = auc_out.datrain.test,
      best_threshold = best_threshold_out.datrain.test,
      confusion_matrix = metrica.format.datrain.test,
      sensitivity = sensitivity_out.datrain.test,
      specificity = specificity_out.datrain.test,
      imp_vars = var_imp_vars.datrain.test
    )
    
    edited.predictor_names <- c(datrain.test.metrics$imp_vars, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss")
    ## second model on all of datrain
    edited.ml_formula <- as.formula(paste0("CL ~ ", paste(edited.predictor_names, collapse="+")))
    edited.mod <- glm(edited.ml_formula, data=datrain, family=binomial)
    
    oob.datest <- predict(edited.mod, newdata=datest, type='response')
    
    auc_out.datest <- pROC::roc(response = datest$CL, predictor= oob.datest, levels=c(0,1), auc = TRUE)
    
    best_threshold_out.datest <- pROC::coords(auc_out.datest, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
    metrica.format.datest <- data.frame(
      cbind(ifelse(datest$CL==1, 1, 0),
            ifelse(oob.datest>=best_threshold_out.datest[1,1], 1, 0))) %>% 
      mutate_all(as.factor)
    colnames(metrica.format.datest) <- c("labels","predictions")
    rownames(metrica.format.datest) <- 1:dim(metrica.format.datest)[1]
    
    metrica.format.datest <- metrica.format.datest$predictions %>% 
      confusionMatrix(positive='1', reference=metrica.format.datest$labels)
    
    sensitivity_out.datest <- caret::recall(data = metrica.format.datest$table, 
                                                  relevant = rownames(metrica.format.datest$table)[2]) 
    specificity_out.datest <- caret::precision(data = metrica.format.datest$table, 
                                                     relevant = rownames(metrica.format.datest$table)[2])
    
    var_imp_vars.datest <- vip::vip(edited.mod)$data$Variable
    
    # for retrieval later
    datest.metrics <- list(
      auc = auc_out.datest,
      best_threshold = best_threshold_out.datest,
      confusion_matrix = metrica.format.datest,
      sensitivity = sensitivity_out.datest,
      specificity = specificity_out.datest,
      imp_vars = var_imp_vars.datest)
    
    saveRDS(datrain.test.metrics,
            paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics.v2/datrain.test/", i, "_", j, "_metrics.rds"))
    saveRDS(datest.metrics,
            paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics.v2/datest/", i, "_", j, "_metrics.rds"))
    saveRDS(edited.mod, paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/models.v2/", i, "_", j, "_edited_mod.rds"))
    }
}
```

Using the above models, let's find a good final threshold and variables to include in the final model, like we did for `balanced_data_v1`.

```{r}
# create empty vectors to be filled
threshold <- c()
imp.vars <- c()
auc <- c()
p_val <- c()

for(j in resume_year:max(balanced_data_v2$Year)){
  for(i in balanced_data_v2$fold %>% unique() %>% sort()){
    # make sure file exists (we excluded several folds in preliminary analysis due to a class being absent)
    if (!file.exists(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics.v2/datest/", i, "_", j, "_metrics.rds"))) {next} 
    
    this.metrics <- readRDS(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics.v2/datest/", i, "_", j, "_metrics.rds"))
    
    threshold <- c(threshold, this.metrics$best_threshold[[1]])
    imp.vars <- c(imp.vars, this.metrics$imp_vars)
    auc <- c(auc, this.metrics$auc$auc[[1]])
    p_val <-c(p_val, this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]])
    
  }
}

# get mean threshold
mean.threshold <- mean(threshold)

# turn imp.vars into data frame
imp.vars.df <- data.frame(var = factor(imp.vars))
# get count of each variable to rank general importance to apply to main data
v2.imp.vars.count <- imp.vars.df %>% 
  count(var) %>% 
  arrange(desc(n))

# get mean auc
mean.auc <- mean(auc)
# get mean accuracy p-value
mean.p_val <- mean(p_val)

metrics_df.balanced_v2 <- data.frame(metric = c('AUC', 'Accuracy P-value', 'Threshold'), value = c(mean.auc, mean.p_val, mean.threshold))
metrics_df.balanced_v2
```

We compare this to the results of `metrics_df.balanced_v1`:

```{r}
metrics_df.balanced_v1
```

```{r}
# change meaning of v2.datrain and v2.datest to balanced.holdout.train and balanced.holdout.test

set.seed(123)
v2.imp_vars <- v2.imp.vars.count %>% 
  select(var) %>% 
  filter(row_number() <= 10) %>% 
  mutate(var = as.character(var))

v2.predictor_names <- c(v2.imp_vars$var, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss") %>%
  unique()

v2.threshold <- metrics_df.balanced_v2 %>% filter(metric=='Threshold') %>% select(value) %>% as.numeric()

v2.ml_formula <- as.formula(paste0("CL ~", paste(v2.predictor_names, collapse=" + ")))

# get testing and training split
split <- initial_split(data, strata = CL)
# v2.datrain <- training(split)
# v2.datest <- testing(split)

v2.datrain <- balanced.holdout.train
v2.datest <- balanced.holdout.test

# get model
v2.mod <- glm(v2.ml_formula, data=v2.datrain, family=binomial)

# calculate out of sample model performance on v2.datest
v2.oob <- predict(v2.mod, newdata=v2.datest, type='response', na.action = na.pass)

v2.temp_auc_out <- pROC::roc(response = v2.datest$CL, predictor= v2.oob, levels=c(0,1), auc = TRUE)

v2.best_threshold_out <- pROC::coords(v2.temp_auc_out, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
v2.metrica.format <- data.frame(
  cbind(ifelse(v2.datest$CL==1, 1, 0),
        ifelse(v2.oob>=v2.best_threshold_out[1,1], 1, 0))) %>% # 0.821
  mutate_all(as.factor)

colnames(v2.metrica.format) <- c("labels","predictions")
rownames(v2.metrica.format) <- 1:dim(v2.metrica.format)[1]

v2.auc_out <- pROC::roc(response = v2.metrica.format$labels, 
                        predictor = v2.metrica.format$predictions %>% as.numeric() - 1,
                        levels=c(0,1), auc = TRUE)

v2.metrica.format.confusionMatrix <- v2.metrica.format$predictions %>% 
  confusionMatrix(positive='1', reference=v2.metrica.format$labels)

v2.sensitivity_out <- caret::recall(data = v2.metrica.format.confusionMatrix$table, 
                                    relevant = rownames(v2.metrica.format.confusionMatrix$table)[2]) 
v2.specificity_out <- caret::precision(data = v2.metrica.format.confusionMatrix$table, 
                                                 relevant = rownames(v2.metrica.format.confusionMatrix$table)[2])

v2.var_imp_vars <- vip::vip(v2.mod, num_features=20)
```

```{r}
v2.var_imp_vars
```


The variable importance plot looks very similar to that of `balanced_data_v1`.


```{r}
v1.var_imp_vars
```


```{r}
v2.incorrect <- v2.datest %>% 
  # filter(row_number() %in% which(v2.metrica.format$labels != v2.metrica.format$predictions))
  mutate(.pred = v2.metrica.format$predictions) %>% 
  mutate(correct = .pred==CL) %>% 
  select(Code, Name, Country, Year, CL, correct, everything())

v2.incorrect %>% summary() 
# there is a 641 to 959 split between true CL absent vs. present

# plots
v2.incorrect.important_vars <- v2.incorrect %>% 
  select(Code, Name, Year, CL, correct, v2.imp_vars$var) %>%
  mutate(log_Population = log(Population)) %>% 
  select(-Population) %>% 
  pivot_longer(cols = min_Elevation:log_Population)

ggplot(v2.incorrect.important_vars) +
  geom_boxplot(aes(x = correct, 
                   y = value)) +
  facet_wrap(facets='name', scales='free')

ggplot(v2.incorrect.important_vars) +
  geom_density(aes(x = value,
                   col = correct)) +
  facet_wrap(facets='name', scales='free')
```

#### Compare the final model performances after getting parameters from balanced_data_v1 compared to balanced_data_v2

```{r}
v1.metrica.format.confusionMatrix
v1.auc_out %>% plot()
v1.auc_out$auc
```

```{r}
v2.metrica.format.confusionMatrix
v2.auc_out %>% plot()
v2.auc_out$auc
```

Now we can return our focus to using a weighted approach to choosing a threshold and predictor variables using `this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]]`. Let's try it on the metrics produced by `balanced_data_v1`

### Weighted approach

For this approach, we get two sets of the top ranked variables based on the AUC and accuracy p-value of their respective models then test them separately.

#### balanced_data_v1

```{r}
# create empty vectors to be filled
threshold <- c()
imp.vars <- data.frame(variable = "", auc = "", p_val = "")
auc <- c()
p_val <- c()

for(j in resume_year:max(balanced_data_v1$Year)){
  for(i in balanced_data_v1$fold %>% unique() %>% sort()){
    # make sure file exists (we excluded several folds in preliminary analysis due to a class being absent)
    if (!file.exists(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datest/", i, "_", j, "_metrics.rds"))) {next} 
    
    this.metrics <- readRDS(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics/datest/", i, "_", j, "_metrics.rds"))
    
    threshold <- c(threshold, this.metrics$best_threshold[[1]])
    imp.vars <- imp.vars %>% 
      rbind(data.frame(variable = this.metrics$imp_vars, auc = this.metrics$auc$auc[[1]], p_val = this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]]))
    auc <- c(auc, this.metrics$auc$auc[[1]])
    p_val <-c(p_val, this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]])
    
  }
}

# get mean threshold
mean.threshold <- mean(threshold)

# turn imp.vars into data frame
imp.vars.df <- data.frame(var = factor(imp.vars))
# get count of each variable to rank general importance to apply to main data
v1.imp.vars.count <- imp.vars.df %>% 
  count(var) %>% 
  arrange(desc(n))

# get mean auc
mean.auc <- mean(auc)
# get mean accuracy p-value
mean.p_val <- mean(p_val)

imp.vars.df.ranked <- imp.vars %>% 
  filter(row_number() > 1) %>% 
  mutate(auc = as.numeric(auc),
         p_val = as.numeric(p_val),
         auc.rank = rank(auc),
         p_val.rank = rank(p_val)) %>% 
  group_by(variable) %>% 
  summarise(mean.auc = mean(auc),
            mean.p_val = mean(p_val)) 

# get auc vars
weighted_v1.auc.vars <- imp.vars.df.ranked %>% 
  arrange(desc(mean.auc))

# get p-val vars
weighted_v1.p_val.vars <- imp.vars.df.ranked %>% 
  arrange((mean.p_val))

metrics_df.balanced_v1 <- data.frame(metric = c('AUC', 'Accuracy P-value', 'Threshold'), value = c(mean.auc, mean.p_val, mean.threshold))
metrics_df.balanced_v1
```

```{r}
# Select the top 10 for each set
weighted_v1.auc.imp_vars <- weighted_v1.auc.vars %>% 
  filter(row_number() <= 10)

weighted_v1.p_val.imp_vars <- weighted_v1.p_val.vars %>% 
  filter(row_number() <= 10)
```

```{r weighted_v1 auc}
# weighted_v1.datrain/datest -> holdout.train/test

set.seed(123)
## auc variables first ##
weighted_v1.auc.predictor_names <- c(weighted_v1.auc.imp_vars$variable, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss") %>%
  unique()

weighted_v1.auc.ml_formula <- as.formula(paste0("CL ~", paste(weighted_v1.auc.predictor_names, collapse=" + ")))

# get testing and training split
# split <- initial_split(data, strata = CL)
# weighted_v1.datrain <- training(split)
# weighted_v1.datest <- testing(split)

weighted_v1.datrain <- balanced.holdout.train
weighted_v1.datest <- balanced.holdout.test

# get model
weighted_v1.auc.mod <- glm(weighted_v1.auc.ml_formula, data=weighted_v1.datrain, family=binomial)

# calculate out of sample model performance on v1.datest
weighted_v1.auc.oob <- predict(weighted_v1.auc.mod, newdata=weighted_v1.datest, type='response', na.action = na.pass)

weighted_v1.auc.temp_auc_out <- pROC::roc(response = weighted_v1.datest$CL, predictor= weighted_v1.auc.oob, levels=c(0,1), auc = TRUE)

weighted_v1.auc.best_threshold_out <- pROC::coords(weighted_v1.auc.temp_auc_out, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
weighted_v1.auc.metrica.format <- data.frame(
  cbind(ifelse(weighted_v1.datest$CL==1, 1, 0),
        ifelse(weighted_v1.auc.oob>=weighted_v1.auc.best_threshold_out[1,1], 1, 0))) %>% # 0.821
  mutate_all(as.factor)

colnames(weighted_v1.auc.metrica.format) <- c("labels","predictions")
rownames(weighted_v1.auc.metrica.format) <- 1:dim(weighted_v1.auc.metrica.format)[1]

weighted_v1.auc.auc_out <- pROC::roc(response = weighted_v1.auc.metrica.format$labels, predictor= weighted_v1.auc.metrica.format$predictions %>% as.numeric() - 1, levels=c(0,1), auc = TRUE)

weighted_v1.auc.metrica.format.confusionMatrix <- weighted_v1.auc.metrica.format$predictions %>% 
  confusionMatrix(positive='1', reference=weighted_v1.auc.metrica.format$labels)

weighted_v1.auc.sensitivity_out <- caret::recall(data = weighted_v1.auc.metrica.format.confusionMatrix$table, 
                                    relevant = rownames(weighted_v1.auc.metrica.format.confusionMatrix$table)[2]) 
weighted_v1.auc.specificity_out <- caret::precision(data = weighted_v1.auc.metrica.format.confusionMatrix$table, 
                                                 relevant = rownames(weighted_v1.auc.metrica.format.confusionMatrix$table)[2])

weighted_v1.auc.var_imp_vars <- vip::vip(weighted_v1.auc.mod)
```

```{r weighted_v1 p_val}
set.seed(123)
## auc variables first ##
weighted_v1.p_val.predictor_names <- c(weighted_v1.p_val.imp_vars$variable, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss") %>%
  unique()

weighted_v1.p_val.ml_formula <- as.formula(paste0("CL ~", paste(weighted_v1.p_val.predictor_names, collapse=" + ")))

# get testing and training split
# split <- initial_split(data, strata = CL)
# weighted_v1.datrain <- training(split)
# weighted_v1.datest <- testing(split)
weighted_1.datrain <- balanced.holdout.train
weighted_v1.datest <- balanced.holdout.test

# get model
weighted_v1.p_val.mod <- glm(weighted_v1.p_val.ml_formula, data=weighted_v1.datrain, family=binomial)

# calculate out of sample model performance on weighted_v1.datest
weighted_v1.p_val.oob <- predict(weighted_v1.p_val.mod, newdata=weighted_v1.datest, type='response', na.action = na.pass)

weighted_v1.p_val.temp_auc_out <- pROC::roc(response = weighted_v1.datest$CL, predictor= weighted_v1.p_val.oob, levels=c(0,1), auc = TRUE)

weighted_v1.p_val.best_threshold_out <- pROC::coords(weighted_v1.p_val.temp_auc_out, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
weighted_v1.p_val.metrica.format <- data.frame(
  cbind(ifelse(weighted_v1.datest$CL==1, 1, 0),
        ifelse(weighted_v1.p_val.oob>=weighted_v1.p_val.best_threshold_out[1,1], 1, 0))) %>% # 0.821
  mutate_all(as.factor)

colnames(weighted_v1.p_val.metrica.format) <- c("labels","predictions")
rownames(weighted_v1.p_val.metrica.format) <- 1:dim(weighted_v1.p_val.metrica.format)[1]

weighted_v1.p_val.auc_out <- pROC::roc(response = weighted_v1.p_val.metrica.format$labels, predictor= weighted_v1.p_val.metrica.format$predictions %>% as.numeric() - 1, levels=c(0,1), auc = TRUE)

weighted_v1.p_val.metrica.format.confusionMatrix <- weighted_v1.p_val.metrica.format$predictions %>% 
  confusionMatrix(positive='1', reference=weighted_v1.p_val.metrica.format$labels)

weighted_v1.p_val.sensitivity_out <- caret::recall(data = weighted_v1.p_val.metrica.format.confusionMatrix$table, 
                                    relevant = rownames(weighted_v1.p_val.metrica.format.confusionMatrix$table)[2]) 
weighted_v1.p_val.specificity_out <- caret::precision(data = weighted_v1.p_val.metrica.format.confusionMatrix$table, 
                                                 relevant = rownames(weighted_v1.p_val.metrica.format.confusionMatrix$table)[2])

weighted_v1.p_val.var_imp_vars <- vip::vip(weighted_v1.p_val.mod)
```

Both models severely underperform when classifying absent points.

#### balanced_data_v2

```{r}
# create empty vectors to be filled
threshold <- c()
imp.vars <- data.frame(variable = "", auc = "", p_val = "")
auc <- c()
p_val <- c()

for(j in resume_year:max(balanced_data_v2$Year)){
  for(i in balanced_data_v2$fold %>% unique() %>% sort()){
    # make sure file exists (we excluded several folds in preliminary analysis due to a class being absent)
    if (!file.exists(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics.v2/datest/", i, "_", j, "_metrics.rds"))) {next} 
    
    this.metrics <- readRDS(paste0("~/peregrine_amazon/Restructured021623/binary_classification/logistic_regression/model_data/st/metrics.v2/datest/", i, "_", j, "_metrics.rds"))
    
    threshold <- c(threshold, this.metrics$best_threshold[[1]])
    imp.vars <- imp.vars %>% 
      rbind(data.frame(variable = this.metrics$imp_vars, auc = this.metrics$auc$auc[[1]], p_val = this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]]))
    auc <- c(auc, this.metrics$auc$auc[[1]])
    p_val <-c(p_val, this.metrics$confusion_matrix$overall["AccuracyPValue"][[1]])
    
  }
}

# get mean threshold
mean.threshold <- mean(threshold)

# turn imp.vars into data frame
imp.vars.df <- data.frame(var = factor(imp.vars))
# get count of each variable to rank general importance to apply to main data
v2.imp.vars.count <- imp.vars.df %>% 
  count(var) %>% 
  arrange(desc(n))

# get mean auc
mean.auc <- mean(auc)
# get mean accuracy p-value
mean.p_val <- mean(p_val)

imp.vars.df.ranked <- imp.vars %>% 
  filter(row_number() > 1) %>% 
  mutate(auc = as.numeric(auc),
         p_val = as.numeric(p_val),
         auc.rank = rank(auc),
         p_val.rank = rank(p_val)) %>% 
  group_by(variable) %>% 
  summarise(mean.auc.rank = mean(auc),
            mean.p_val.rank = mean(p_val)) 

weighted_v2.auc.vars <- imp.vars.df.ranked %>% 
  arrange(desc(mean.auc.rank))

weighted_v2.p_val.vars <- imp.vars.df.ranked %>% 
  arrange((mean.p_val.rank))

metrics_df.balanced_v2 <- data.frame(metric = c('AUC', 'Accuracy P-value', 'Threshold'), value = c(mean.auc, mean.p_val, mean.threshold))
metrics_df.balanced_v2
```

```{r}
weighted_v2.auc.imp_vars <- weighted_v2.auc.vars %>% 
  filter(row_number() <= 10)

weighted_v2.p_val.imp_vars <- weighted_v2.p_val.vars %>% 
  filter(row_number() <= 10)
```

```{r weighted_v2 auc}
set.seed(123)

## auc variables first ##
weighted_v2.auc.predictor_names <- c(weighted_v2.auc.imp_vars$variable, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss") %>%
  unique()

weighted_v2.auc.ml_formula <- as.formula(paste0("CL ~", paste(weighted_v2.auc.predictor_names, collapse=" + ")))

# get testing and training split
split <- initial_split(data, strata = CL)
# weighted_v2.datrain <- training(split)
# weighted_v2.datest <- testing(split)
weighted_v2.datrain <- balanced.holdout.train
weighted_v2.datest <- balanced.holdout.test

# get model
weighted_v2.auc.mod <- glm(weighted_v2.auc.ml_formula, data=weighted_v2.datrain, family=binomial)

# calculate out of sample model performance on weighted_v2.datest
weighted_v2.auc.oob <- predict(weighted_v2.auc.mod, newdata=weighted_v2.datest, type='response', na.action = na.pass)

weighted_v2.auc.temp_auc_out <- pROC::roc(response = weighted_v2.datest$CL, predictor= weighted_v2.auc.oob, levels=c(0,1), auc = TRUE)

weighted_v2.auc.best_threshold_out <- pROC::coords(weighted_v2.auc.temp_auc_out, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
weighted_v2.auc.metrica.format <- data.frame(
  cbind(ifelse(weighted_v2.datest$CL==1, 1, 0),
        ifelse(weighted_v2.auc.oob>=weighted_v2.auc.best_threshold_out[1,1], 1, 0))) %>% # 0.821
  mutate_all(as.factor)

colnames(weighted_v2.auc.metrica.format) <- c("labels","predictions")
rownames(weighted_v2.auc.metrica.format) <- 1:dim(weighted_v2.auc.metrica.format)[1]

weighted_v2.auc.auc_out <- pROC::roc(response = weighted_v2.auc.metrica.format$labels, predictor= weighted_v2.auc.metrica.format$predictions %>% as.numeric() - 1, levels=c(0,1), auc = TRUE)

weighted_v2.auc.metrica.format.confusionMatrix <- weighted_v2.auc.metrica.format$predictions %>% 
  confusionMatrix(positive='1', reference=weighted_v2.auc.metrica.format$labels)

weighted_v2.auc.sensitivity_out <- caret::recall(data = weighted_v2.auc.metrica.format.confusionMatrix$table, 
                                    relevant = rownames(weighted_v2.auc.metrica.format.confusionMatrix$table)[2]) 
weighted_v2.auc.specificity_out <- caret::precision(data = weighted_v2.auc.metrica.format.confusionMatrix$table, 
                                                 relevant = rownames(weighted_v2.auc.metrica.format.confusionMatrix$table)[2])

weighted_v2.auc.var_imp_vars <- vip::vip(weighted_v2.auc.mod)
```

```{r weighted_v2 p_val}
set.seed(123)
## auc variables first ##
weighted_v2.p_val.predictor_names <- c(weighted_v2.p_val.imp_vars$variable, "forest_density", "forest_fragmentation", "land_use_change", "edge_loss") %>%
  unique()

weighted_v2.p_val.ml_formula <- as.formula(paste0("CL ~", paste(weighted_v2.p_val.predictor_names, collapse=" + ")))

# get testing and training split
split <- initial_split(data, strata = CL)
# weighted_v2.datrain <- training(split)
# weighted_v2.datest <- testing(split)
weighted_v2.datrain <- balanced.holdout.train
weighted_v2.datest <- balanced.holdout.test

# get model
weighted_v2.p_val.mod <- glm(weighted_v2.p_val.ml_formula, data=weighted_v2.datrain, family=binomial)

# calculate out of sample model performance on weighted_v2.datest
weighted_v2.p_val.oob <- predict(weighted_v2.p_val.mod, newdata=weighted_v2.datest, type='response', na.action = na.pass)

weighted_v2.p_val.temp_auc_out <- pROC::roc(response = weighted_v2.datest$CL, predictor= weighted_v2.p_val.oob, levels=c(0,1), auc = TRUE)

weighted_v2.p_val.best_threshold_out <- pROC::coords(weighted_v2.p_val.temp_auc_out, "best", ret = "threshold") # save this threshold; want: take average of threshold across all spatio-temporal folds
weighted_v2.p_val.metrica.format <- data.frame(
  cbind(ifelse(weighted_v2.datest$CL==1, 1, 0),
        ifelse(weighted_v2.p_val.oob>=weighted_v2.p_val.best_threshold_out[1,1], 1, 0))) %>% # 0.821
  mutate_all(as.factor)

colnames(weighted_v2.p_val.metrica.format) <- c("labels","predictions")
rownames(weighted_v2.p_val.metrica.format) <- 1:dim(weighted_v2.p_val.metrica.format)[1]

weighted_v2.p_val.auc_out <- pROC::roc(response = weighted_v2.p_val.metrica.format$labels, predictor= weighted_v2.p_val.metrica.format$predictions %>% as.numeric() - 1, levels=c(0,1), auc = TRUE)

weighted_v2.p_val.metrica.format.confusionMatrix <- weighted_v2.p_val.metrica.format$predictions %>% 
  confusionMatrix(positive='1', reference=weighted_v2.p_val.metrica.format$labels)

weighted_v2.p_val.sensitivity_out <- caret::recall(data = weighted_v2.p_val.metrica.format.confusionMatrix$table, 
                                    relevant = rownames(weighted_v2.p_val.metrica.format.confusionMatrix$table)[2]) 
weighted_v2.p_val.specificity_out <- caret::precision(data = weighted_v2.p_val.metrica.format.confusionMatrix$table, 
                                                 relevant = rownames(weighted_v2.p_val.metrica.format.confusionMatrix$table)[2])

weighted_v2.p_val.var_imp_vars <- vip::vip(weighted_v2.p_val.mod)
```


## Results

We open the results section with the confusion matrices and AUCs of all models.

Full variables:

```{r}
full.metrica.format.confusionMatrix
full.auc_out
```

First unweighted resampling approach (v1):

```{r}
v1.metrica.format.confusionMatrix
v1.auc_out
```

Second unweighted resampling approach (v2):

```{r}
v2.metrica.format.confusionMatrix
v2.auc_out
```

First weighted variables approach (v1):

```{r}
weighted_v1.auc.metrica.format.confusionMatrix
weighted_v1.auc.auc_out

weighted_v1.p_val.metrica.format.confusionMatrix
weighted_v1.p_val.auc_out
```

Second weighted variables approach (v2):

```{r}
weighted_v2.auc.metrica.format.confusionMatrix
weighted_v2.auc.auc_out

weighted_v2.p_val.metrica.format.confusionMatrix
weighted_v2.p_val.auc_out
```

Using the weighted variable approaches give very poor performances for predicting absences. The objective "best" performing model is the model using all variables, but close behind are the unweighted variable approaches. These should take the place of the full variable approach since they use less variables, which improves generalizability. 

```{r, fig.height=12, fig.height=12}
full.incorrect <- full.datest %>% 
  # filter(row_number() %in% which(full.metrica.format$labels != full.metrica.format$predictions))
  mutate(.pred = full.metrica.format$predictions) %>% 
  mutate(correct = .pred==CL) %>% 
  select(Code, Name, Country, Year, CL, .pred, correct, everything())

full.incorrect %>% summary() 

# plots
full.incorrect.important_vars <- full.incorrect %>% 
  select(Code, Name, Year, CL, correct, everything()) %>%
  mutate(log_Population = log(Population)) %>% 
  select(-Population) %>% 
  pivot_longer(cols = NDVI:log_Population)

ggplot(full.incorrect.important_vars) +
  geom_boxplot(aes(x = correct, 
                   y = value)) +
  facet_wrap(facets='name', scales='free', ncol = 3)

ggplot(full.incorrect.important_vars) +
  geom_density(aes(x = value,
                   col = correct)) +
  facet_wrap(facets='name', scales='free', ncol = 3)
```

#### Get spatial distribution of correctly and incorrectly classified municipios

```{r}
# get a data frame that gives a percentage of how many instances each municipio was classified incorrectly
percent_wrong.df <- full.incorrect %>% 
  select(Code, Year, Country, correct, 
         land_use_change, forest_density, forest_fragmentation, edge_loss, mean_Elevation) %>% 
  group_by(Code) %>% # group by Code when summarizing since number of observations per municipio differ by country and code
  mutate(num_years = unique(Year) %>% length(),
         num_correct = sum(correct)) %>% 
  summarise(percent_wrong = 1 - (num_correct/num_years),
            median_deforested = median(land_use_change),
            max_forest_change = max(forest_density) - min(forest_density),
            mean_forest_fragmentation = mean(forest_fragmentation),
            mean_edge_loss = mean(edge_loss),
            mean_Elevation = mean(mean_Elevation)) %>% 
  ungroup()

geom.df <- readRDS("~/peregrine_amazon/data/annual/aad_2021_forests_geom.rds") %>% 
  select(Code, geometry) %>% 
  unique()

percent_wrong.geom.df <- percent_wrong.df %>% 
  left_join(geom.df, by='Code') %>% 
  filter(!is.na(percent_wrong)) %>% 
  sf::st_as_sf() %>% 
  unique()

percent_wrong.all_incorrect.df <- percent_wrong.geom.df %>% 
  filter(percent_wrong == 1)

ggplot(percent_wrong.all_incorrect.df) +
  geom_density(aes(x=max_forest_change))

percent_wrong.no_correct.df <- percent_wrong.geom.df %>% 
  filter(percent_wrong > 0)

ggplot(percent_wrong.no_correct.df) +
  geom_density(aes(x=max_forest_change))

# maps; change parameters in zcol 
mapview::mapview(percent_wrong.all_incorrect.df, zcol = 'mean_forest_fragmentation')
mapview::mapview(percent_wrong.no_correct.df, zcol = 'mean_forest_fragmentation')


percent_wrong.no_correct.df %>% summary()
percent_wrong.all_incorrect.df %>% summary()
```

